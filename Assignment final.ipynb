{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Final Assignment  â€“ Marc Glowienke\n",
    "\n",
    "To properly run this notebook, the data has to be downloaded from https://drive.google.com/file/d/1G_Exgw9WXI6swzGQEyzueOQvB_01mUJt/view to be in the correct form and folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preamble\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.signal import decimate\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "def get_dataset_name(file_name_with_dir): # Function used to load files\n",
    "    filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = \"_\".join(temp)\n",
    "    return dataset_name\n",
    "\n",
    "def load_intra():\n",
    "    ## Train data\n",
    "    x = []\n",
    "    y = []\n",
    "    paths = [\"Final_Project_DL/Intra/train/rest_105923_%.0f.h5\",\"Final_Project_DL/Intra/train/task_motor_105923_%.0f.h5\",\n",
    "             \"Final_Project_DL/Intra/train/task_story_math_105923_%.0f.h5\",\n",
    "             \"Final_Project_DL/Intra/train/task_working_memory_105923_%.0f.h5\"]\n",
    "    for path in paths:\n",
    "        for k in range(1,9):\n",
    "            filename_path= path %k\n",
    "            with h5py.File(filename_path,'r') as f:\n",
    "                dataset_name = get_dataset_name(filename_path)\n",
    "                matrix = f.get(dataset_name)[()]\n",
    "                x.append(matrix) #create x frame\n",
    "\n",
    "                # transfrom class to number code\n",
    "                if dataset_name.split(\"_\")[-2] == 'rest':\n",
    "                    Class = 0\n",
    "                elif dataset_name.split(\"_\")[-2] == 'motor':\n",
    "                    Class = 1\n",
    "                elif dataset_name.split(\"_\")[-2] == 'math':\n",
    "                    Class = 2\n",
    "                elif dataset_name.split(\"_\")[-2] == 'memory':\n",
    "                    Class = 3\n",
    "                else: print('Error in adding classes')\n",
    "\n",
    "                y.append([int(dataset_name.split(\"_\")[-1]),Class])\n",
    "    intra_train_x = np.array(x)\n",
    "    intra_train_y = np.array(y)\n",
    "    del x,y\n",
    "    ## Test data\n",
    "    x = []\n",
    "    y = []\n",
    "    paths = [\"Final_Project_DL/Intra/test/rest_105923_%.0f.h5\",\"Final_Project_DL/Intra/test/task_motor_105923_%.0f.h5\",\n",
    "             \"Final_Project_DL/Intra/test/task_story_math_105923_%.0f.h5\",\n",
    "             \"Final_Project_DL/Intra/test/task_working_memory_105923_%.0f.h5\"]\n",
    "    for path in paths:\n",
    "        for k in range(9,11):\n",
    "            filename_path= path %k\n",
    "            with h5py.File(filename_path,'r') as f:\n",
    "                dataset_name = get_dataset_name(filename_path)\n",
    "                matrix = f.get(dataset_name)[()]\n",
    "                x.append(matrix) #create x frame\n",
    "\n",
    "                # transfrom class to number code\n",
    "                if dataset_name.split(\"_\")[-2] == 'rest':\n",
    "                    Class = 0\n",
    "                elif dataset_name.split(\"_\")[-2] == 'motor':\n",
    "                    Class = 1\n",
    "                elif dataset_name.split(\"_\")[-2] == 'math':\n",
    "                    Class = 2\n",
    "                elif dataset_name.split(\"_\")[-2] == 'memory':\n",
    "                    Class = 3\n",
    "                else: print('Error in adding classes')\n",
    "\n",
    "                y.append([int(dataset_name.split(\"_\")[-1]),Class])\n",
    "    intra_test_x = np.array(x)\n",
    "    intra_test_y = np.array(y)\n",
    "    del x,y\n",
    "\n",
    "    print(\"Intra: \",intra_train_x.shape, intra_train_y.shape, intra_test_x.shape, intra_test_y.shape)\n",
    "    return intra_train_x, intra_train_y, intra_test_x, intra_test_y\n",
    "\n",
    "def path_list(file_path):\n",
    "    train_paths = []\n",
    "    d = file_path\n",
    "    for path in os.listdir(d):\n",
    "        full_path = os.path.join(d, path)\n",
    "        if os.path.isfile(full_path):\n",
    "            train_paths.append(full_path)\n",
    "    return train_paths\n",
    "\n",
    "\n",
    "def concatenate_data(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for path in data_path:\n",
    "        with h5py.File(path,'r') as f:\n",
    "            dataset_name = get_dataset_name(path)\n",
    "            matrix = f.get(dataset_name)[()]\n",
    "            data.append(matrix)\n",
    "\n",
    "            if dataset_name.split(\"_\")[-2] == 'rest':\n",
    "                Class = 0\n",
    "            elif dataset_name.split(\"_\")[-2] == 'motor':\n",
    "                Class = 1\n",
    "            elif dataset_name.split(\"_\")[-2] == 'math':\n",
    "                Class = 2\n",
    "            elif dataset_name.split(\"_\")[-2] == 'memory':\n",
    "                Class = 3\n",
    "\n",
    "            labels.append([int(dataset_name.split(\"_\")[-1]),Class])\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def load_cross():\n",
    "    train_path = path_list('Final_Project_DL/Cross/train/')\n",
    "    train_data, train_data_labels = concatenate_data(train_path)\n",
    "\n",
    "    old_test1_path = sorted(path_list('Final_Project_DL/Cross/test1/'))\n",
    "    test1_order = [0, 2, 3, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    test1_path = [old_test1_path[i] for i in test1_order]\n",
    "    test1_data, test1_data_labels = concatenate_data(test1_path)\n",
    "\n",
    "    old_test2_path = sorted(path_list('Final_Project_DL/Cross/test2/'))\n",
    "    test2_order = [0, 2, 3, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    test2_path = [old_test2_path[i] for i in test2_order]\n",
    "    test2_data, test2_data_labels = concatenate_data(test2_path)\n",
    "\n",
    "    old_test3_path = sorted(path_list('Final_Project_DL/Cross/test3/'))\n",
    "    test3_order = [0, 2, 3, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    test3_path = [old_test3_path[i] for i in test3_order]\n",
    "    test3_data, test3_data_labels = concatenate_data(test3_path)\n",
    "\n",
    "    print(train_data.shape,train_data_labels.shape,test1_data.shape,test1_data_labels.shape,\n",
    "          test2_data.shape,test2_data_labels.shape,test3_data.shape,test3_data_labels.shape)\n",
    "    return train_data,train_data_labels,test1_data,test1_data_labels,test2_data,test2_data_labels,test3_data,test3_data_labels\n",
    "\n",
    "def data_wrangle(train_x,train_y,DOWNSAMPLE_RATE):\n",
    "    train_x = decimate(train_x,DOWNSAMPLE_RATE) # decrease the sampling rate\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_y[:,1]) # make labels to 1-hot encoding\n",
    "\n",
    "    ## Scaling per chunk to [0,1], so all channels are scaled using the same min and max per chunk\n",
    "    train_x_scaled = np.empty(train_x.shape)\n",
    "    for chunk in range(train_x.shape[0]):\n",
    "        maxval = np.amax(train_x[chunk,:,:])\n",
    "        minval = np.amin(train_x[chunk,:,:])\n",
    "        train_x_scaled[chunk,:,:] = ((train_x[chunk,:,:] - minval)/(maxval - minval))\n",
    "\n",
    "    train_x, train_labels = shuffle(train_x_scaled,train_labels,random_state = 0) # shuffle samples and labels\n",
    "    train_x = np.swapaxes(train_x,1,2) # change to shape, where features are last\n",
    "\n",
    "\n",
    "    print(\"Wrangle: \",train_x.shape, train_labels.shape)\n",
    "    return train_x, train_labels\n",
    "\n",
    "def data_windows(x,y, window_size): #Split data into windows of certain length,\n",
    "    data = np.empty((1,WINDOW_SIZE,248)) # Ommit end part to have equal length\n",
    "    labels = np.empty((1,4))\n",
    "\n",
    "    for chunk in range(x.shape[0]):\n",
    "        for k in range(WINDOW_SIZE,int(np.floor(x.shape[1]/WINDOW_SIZE)*WINDOW_SIZE)+1,WINDOW_SIZE):\n",
    "            indices = range(k - WINDOW_SIZE,k)\n",
    "            sample = x[chunk,indices,:]\n",
    "            data = np.append(data,sample.reshape(1,WINDOW_SIZE,x.shape[2]),axis = 0)\n",
    "            labels = np.append(labels,y[chunk].reshape((1,4)),axis=0)\n",
    "    print(\"WINDOW: \",data[1:,:,:].shape, labels[1:,:].shape)\n",
    "\n",
    "    X, Y = shuffle(data[1:,:,:], labels[1:,:],random_state = 0)\n",
    "    return X, Y\n",
    "\n",
    "#### MODEL\n",
    "def train_lstm(train_x,train_labels,epochs,batch_size):\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint('best_model_lstm.h5', monitor='val_accuracy', mode='max',verbose = 1, save_best_only=True)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LSTM(200,input_shape=train_x.shape[-2:]),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(100, activation='relu'),\n",
    "        tf.keras.layers.Dense(100, activation='relu'),\n",
    "        tf.keras.layers.Dense(train_labels.shape[1], activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    history = model.fit(train_x, train_labels, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_split = 0.2,callbacks = [mc],verbose = 0)\n",
    "    model = tf.keras.models.load_model('best_model_lstm.h5')\n",
    "\n",
    "    print(\"---------------------------------------- \\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy_plot = history.history['accuracy']\n",
    "    plt.figure(figsize= (20,10))\n",
    "    plt.plot(range(len(loss)), loss, 'b', label='Training loss')\n",
    "    plt.plot(range(len(loss)), accuracy_plot, 'r', label='Accuracy')\n",
    "    plt.plot(range(len(loss)), val_loss, 'orange', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss for LSTM Model\")\n",
    "    plt.savefig('loss_lstm.pdf',bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_lstm(model,test_x,test_labels,batch_size):\n",
    "    _, accuracy = model.evaluate(test_x, test_labels, batch_size=batch_size, verbose=0)\n",
    "    print(\"---------------------------------------- \\n\")\n",
    "\n",
    "    predictions = tf.argmax(model.predict(test_x), axis = 1)\n",
    "    labels = tf.argmax(test_labels, axis = 1)\n",
    "    confusion_matrix = tf.math.confusion_matrix(\n",
    "        labels, predictions, num_classes=None, weights=None, dtype=tf.dtypes.int32, name=None)\n",
    "\n",
    "    return accuracy, confusion_matrix\n",
    "\n",
    "\n",
    "def train_cnn(train_x,train_labels,epochs,batch_size,filters,kernels):\n",
    "    # define model\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint('best_model_cnn.h5', monitor='val_accuracy', mode='max',verbose = 1, save_best_only=True)\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernels, activation='relu', input_shape=train_x.shape[-2:]),\n",
    "    tf.keras.layers.Conv1D(filters=filters, kernel_size=kernels, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(train_labels.shape[1], activation='softmax'),\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(train_x, train_labels, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_split = 0.2,callbacks = [mc],verbose = 0)\n",
    "    \n",
    "    model = tf.keras.models.load_model('best_model_cnn.h5')\n",
    "\n",
    "    print(\"---------------------------------------- \\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy_plot = history.history['accuracy']\n",
    "    plt.figure(figsize= (20,10))\n",
    "    plt.plot(range(len(loss)), loss, 'b', label='Training loss')\n",
    "    plt.plot(range(len(loss)), accuracy_plot, 'r', label='Accuracy')\n",
    "    plt.plot(range(len(loss)), val_loss, 'orange', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss for CNN Model\")\n",
    "    plt.savefig('loss_cnn.pdf',bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_cnn(model,test_x,test_labels,batch_size):\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(test_x, test_labels, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    print(\"---------------------------------------- \\n\")\n",
    "\n",
    "    predictions = tf.argmax(model.predict(test_x), axis = 1)\n",
    "    labels = tf.argmax(test_labels, axis = 1)\n",
    "    confusion_matrix = tf.math.confusion_matrix(\n",
    "        labels, predictions, num_classes=None, weights=None, dtype=tf.dtypes.int32, name=None)\n",
    "\n",
    "    return accuracy,confusion_matrix\n",
    "\n",
    "accuracy_lstm1 = None\n",
    "accuracy_cnn1 = None\n",
    "accuracy_cnn = None\n",
    "accuracy_lstm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN\n",
    "### HYPERPARAMETER COLLECTION\n",
    "DOWNSAMPLE_RATE = 10\n",
    "EPOCHS_LSTM = 50\n",
    "EPOCHS_CNN = 100\n",
    "BATCH_SIZE = 68\n",
    "WINDOW_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_train_x, intra_train_y, intra_test_x, intra_test_y = load_intra()\n",
    "\n",
    "train_x, train_labels = data_wrangle(intra_train_x, intra_train_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "test_x, test_labels = data_wrangle(intra_test_x, intra_test_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "intra_train_x, intra_train_labels = data_windows(train_x,train_labels,window_size = WINDOW_SIZE)\n",
    "intra_test_x, intra_test_labels = data_windows(test_x, test_labels,window_size = WINDOW_SIZE)\n",
    "del intra_train_y,intra_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_train_x, cross_train_y, cross_test1_x, cross_test1_y, cross_test2_x, cross_test2_y, cross_test3_x, cross_test3_y = load_cross()\n",
    "train_x, train_labels = data_wrangle(cross_train_x, cross_train_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "train_x, train_labels = data_windows(train_x,train_labels,window_size = WINDOW_SIZE)\n",
    "del cross_train_x, cross_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_labels = data_wrangle(cross_test1_x, cross_test1_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "test1_x, test1_labels = data_windows(test_x, test_labels,window_size = WINDOW_SIZE)\n",
    "\n",
    "test_x, test_labels = data_wrangle(cross_test2_x, cross_test2_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "test2_x, test2_labels = data_windows(test_x, test_labels,window_size = WINDOW_SIZE)\n",
    "\n",
    "test_x, test_labels = data_wrangle(cross_test3_x, cross_test3_y,DOWNSAMPLE_RATE = DOWNSAMPLE_RATE)\n",
    "test3_x, test3_labels = data_windows(test_x,test_labels,window_size = WINDOW_SIZE)\n",
    "del test_x, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------------------------------------- \\n\")\n",
    "model_lstm = train_lstm(intra_train_x,intra_train_labels,epochs = EPOCHS_LSTM,batch_size = BATCH_SIZE)\n",
    "\n",
    "accuracy_lstm,confusion_matrix_lstm = evaluate_lstm(model_lstm,intra_test_x,intra_test_labels,batch_size = BATCH_SIZE)\n",
    "model_cnn = train_cnn(intra_train_x,intra_train_labels,epochs = EPOCHS_CNN,batch_size = BATCH_SIZE,filters = 64,kernels = 3)\n",
    "accuracy_cnn,confusion_matrix_cnn = evaluate_cnn(model_cnn,intra_test_x,intra_test_labels,batch_size = BATCH_SIZE)\n",
    "print(\"---------------------------------------- \\n\")\n",
    "if accuracy_lstm != None:\n",
    "    print(\"RESULTS LSTM: \\n\")\n",
    "    print(\"epochs: %.0f\" %EPOCHS_LSTM)\n",
    "    print(\"Accuracy on the test set for LSTM %.4f \\n\" %accuracy_lstm)\n",
    "    print(\"\\n Confusion Matrix LSTM:\")\n",
    "    print(confusion_matrix_lstm)\n",
    "if accuracy_cnn != None:\n",
    "    print(\"RESULTS CNN: \\n\")\n",
    "    print(\"epochs: %.0f \\n\" %EPOCHS_CNN)\n",
    "    print(\"Accuracy on the test set for CNN %.4f\" %accuracy_cnn)\n",
    "    print(\"\\n Confusion Matrix CNN:\")\n",
    "    print(confusion_matrix_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm = train_lstm(train_x,train_labels,epochs = EPOCHS_LSTM,batch_size = BATCH_SIZE)\n",
    "model_cnn = train_cnn(train_x,train_labels,epochs = EPOCHS_CNN,batch_size = BATCH_SIZE, filters = 64,kernels = 3)\n",
    "\n",
    "accuracy_lstm1,confusion_matrix_lstm1 = evaluate_lstm(model_lstm,test1_x,test1_labels,batch_size = BATCH_SIZE)\n",
    "accuracy_cnn1,confusion_matrix_cnn1 = evaluate_cnn(model_cnn,test1_x,test1_labels,batch_size = BATCH_SIZE)\n",
    "\n",
    "accuracy_lstm2,confusion_matrix_lstm2 = evaluate_lstm(model_lstm,test2_x,test2_labels,batch_size = BATCH_SIZE)\n",
    "accuracy_cnn2,confusion_matrix_cnn2 = evaluate_cnn(model_cnn,test2_x,test2_labels,batch_size = BATCH_SIZE)\n",
    "\n",
    "accuracy_lstm3,confusion_matrix_lstm3 = evaluate_lstm(model_lstm,test3_x,test3_labels,batch_size = BATCH_SIZE)\n",
    "accuracy_cnn3,confusion_matrix_cnn3 = evaluate_cnn(model_cnn,test3_x,test3_labels,batch_size = BATCH_SIZE)\n",
    "\n",
    "print(\"---------------------------------------- \\n\")\n",
    "if accuracy_lstm1 != None:\n",
    "    print(\"RESULTS LSTM: \\n\")\n",
    "    print(\"epochs: %.0f\" %EPOCHS_LSTM)\n",
    "    print(\"Accuracy on test set 1 for LSTM %.4f \\n\" %accuracy_lstm1)\n",
    "    print(\"Accuracy on test set 2 for LSTM %.4f \\n\" %accuracy_lstm2)\n",
    "    print(\"Accuracy on test set 3 for LSTM %.4f \\n\" %accuracy_lstm3)\n",
    "    print(\"\\n Confusion Matrix LSTM 1:\")\n",
    "    print(confusion_matrix_lstm1)\n",
    "    print(\"\\n Confusion Matrix LSTM 2:\")\n",
    "    print(confusion_matrix_lstm2)\n",
    "    print(\"\\n Confusion Matrix LSTM 3:\")\n",
    "    print(confusion_matrix_lstm3)\n",
    "if accuracy_cnn1 != None:\n",
    "    print(\"RESULTS CNN: \\n\")\n",
    "    print(\"epochs: %.0f \\n\" %EPOCHS_CNN)\n",
    "    print(\"Accuracy on test set 1 for CNN %.4f \\n\" %accuracy_cnn1)\n",
    "    print(\"Accuracy on test set 2 for CNN %.4f \\n\" %accuracy_cnn2)\n",
    "    print(\"Accuracy on test set 3 for CNN %.4f \\n\" %accuracy_cnn3)\n",
    "    print(\"\\n Confusion Matrix CNN 1:\")\n",
    "    print(confusion_matrix_cnn1)\n",
    "    print(\"\\n Confusion Matrix CNN 2:\")\n",
    "    print(confusion_matrix_cnn2)\n",
    "    print(\"\\n Confusion Matrix CNN 3:\")\n",
    "    print(confusion_matrix_cnn3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
